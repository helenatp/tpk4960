{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "from functools import wraps\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy.random import randn, rand\n",
    "import numdifftools as nd\n",
    "import picos as pc\n",
    "from scipy.integrate import solve_ivp\n",
    "from scipy.io import loadmat\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "plt.style.use(\"default\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runtime(func):\n",
    "    @wraps(func)\n",
    "    def runtime_wrapper(*args, **kwargs):\n",
    "        start_time = time.perf_counter()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.perf_counter()\n",
    "        total_time = end_time - start_time\n",
    "        print(\"\\n\", f'Function {func.__name__} took {total_time:.4f} seconds')\n",
    "        return result\n",
    "    return runtime_wrapper"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_of_dataset = 'Angle'\n",
    "#name_of_dataset = 'Sshape'\n",
    "\n",
    "dataset = loadmat(\"Dataset/\" + name_of_dataset + '.mat')\n",
    "# Unpack the trajectories and place into x and y used in learning step\n",
    "num_of_demos = dataset['demos'].shape[1]\n",
    "size_of_state = dataset['demos'][0, 0][0,0][0].shape[0]\n",
    "dim_of_function = size_of_state "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_train_datasets = 4\n",
    "num_of_points = len(dataset['demos'][0,0][0,0][0][0])\n",
    "\n",
    "pos_cell_1 = np.zeros((num_of_points, num_of_train_datasets))\n",
    "pos_cell_2 = np.zeros((num_of_points, num_of_train_datasets))\n",
    "vel_cell_1 = np.zeros((num_of_points, num_of_train_datasets))\n",
    "vel_cell_2 = np.zeros((num_of_points, num_of_train_datasets))\n",
    "\n",
    "# Concatenating the dataseries of position and velocities\n",
    "for i in range(0, num_of_train_datasets):\n",
    "    demo_struct = dataset['demos'][0,i][0,0]\n",
    "\n",
    "    position_dataseries = dataset['demos'][0,i][0,0][0]\n",
    "    velocity_dataseries = dataset['demos'][0,i][0,0][2]\n",
    "\n",
    "    pos_cell_1[:, i] = position_dataseries[0]\n",
    "    pos_cell_2[:, i] = position_dataseries[1]\n",
    "    vel_cell_1[:, i] = velocity_dataseries[0]\n",
    "    vel_cell_2[:, i] = velocity_dataseries[1]\n",
    "\n",
    "x_train_1 = pos_cell_1.mean(axis=1)\n",
    "x_train_2 = pos_cell_2.mean(axis=1)\n",
    "y_train_1 = vel_cell_1.mean(axis=1)\n",
    "y_train_2 = vel_cell_2.mean(axis=1)\n",
    "\n",
    "# Defines the position and velocity measurements\n",
    "x_train = np.array([x_train_1, x_train_2])\n",
    "y_train = np.array([y_train_1, y_train_2])\n",
    "\n",
    "# Get time info for simulating the learnt system later\n",
    "time_step = dataset['dt'][0,0]\n",
    "\n",
    "len_of_dataseries = dataset['demos'][0, 0][0,0][0].shape[1] \n",
    "\n",
    "concatenate = np.concatenate((np.array([[0]]), time_step * np.ones((1, len_of_dataseries - 1))),axis=1)\n",
    "time_series = np.cumsum(concatenate)\n",
    "num_of_timesteps = len_of_dataseries\n",
    "num_of_test_datasets = num_of_demos - num_of_train_datasets\n",
    "\n",
    "# Intial starting point for the simulated trajectory using learnt model\n",
    "init_condition = x_train[:, 0]\n",
    "\n",
    "timespan = np.array([time_series[0],time_series[-1]])\n",
    "t_series = np.linspace(time_series[0],time_series[-1], 1000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot state trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({\n",
    "    \"text.usetex\": False,\n",
    "    \"font.family\": \"serif\",\n",
    "    \"font.serif\": \"Times New Roman\",\n",
    "})\n",
    "\n",
    "fig_dataset = plt.figure(1)\n",
    "plt.title('Trajectory used in regression problem - ' + name_of_dataset, fontsize=14)\n",
    "plt.plot(x_train[0,:], x_train[1,:], '.')\n",
    "plt.xlabel('x1', fontsize =12)\n",
    "plt.ylabel('x2', fontsize=12)\n",
    "fig_dataset.show()\n",
    "#plt.savefig(r\"lasa_figures/\"+name_of_dataset+\"_lasa_vanish_trajectory.eps\", format =\"eps\")\n",
    "\n",
    "axes = plt.gca() #gca = get current axes, from my last plot"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define vector-field and axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get lower and upper bound for plotting from dataset figure\n",
    "lower_bound_x1 = axes.get_xlim()[0]\n",
    "upper_bound_x1 = axes.get_xlim()[1]\n",
    "lower_bound_x2 = axes.get_ylim()[0]\n",
    "upper_bound_x2 = axes.get_ylim()[1]\n",
    "\n",
    "num_of_points_for_plot = 25\n",
    "\n",
    "x1 = np.linspace(lower_bound_x1-5, upper_bound_x1, num_of_points_for_plot)\n",
    "x2 = np.linspace(lower_bound_x2, upper_bound_x2+4, num_of_points_for_plot)\n",
    "\n",
    "# Base point or starting point for each vector of the vector field\n",
    "[X1, X2] = np.meshgrid(x1, x2)\n",
    "\n",
    "t = 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector-valued RFF for Gaussian Separable Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def psi(x, w, b):\n",
    "    return np.sqrt(2) * np.cos(w.T@x + b)\n",
    "\n",
    "def psi_vector(x, w, b, d):\n",
    "    psi_vector = np.zeros(d)\n",
    "    for i in range(d):\n",
    "        x = x.reshape((-1,))\n",
    "        psi_vector[i] = psi(x, w[:, i], b[:,i])\n",
    "    return np.array(psi_vector)\n",
    "\n",
    "def capital_psi(x, w, b, d, dim):\n",
    "    psi = psi_vector(x, w, b, d)\n",
    "    return np.kron(psi, np.eye(dim))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = 10\n",
    "lambda_ = 0.1\n",
    "num_of_constraint_points = 250\n",
    "num_of_samples = 100\n",
    "\n",
    "if name_of_dataset == 'Sshape':\n",
    "    num_of_samples = 200\n",
    "\n",
    "#w = randn(size_of_state, num_of_samples)/sigma\n",
    "#b = rand(1,num_of_samples)*2*np.pi\n",
    "\n",
    "w = np.load('RFF_parameters/w_'+name_of_dataset+'_vanish.npy')\n",
    "b = np.load('RFF_parameters/b_'+name_of_dataset+'_vanish.npy')\n",
    "\n",
    "mu = 0.0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Features Vanishing on a Point Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phi_function(x, w, b, N, d, dim):\n",
    "    phi = np.zeros((dim*N, dim*d))\n",
    "    psi = capital_psi(x, w, b, d, dim)\n",
    "    phi[dim*0] = psi[0]\n",
    "    phi[dim*0+1] = psi[1]\n",
    "    return phi\n",
    "\n",
    "def capital_psi_z_function(x, w, b, d, dim, L):\n",
    "    psi = capital_psi(x, w, b, d, dim)\n",
    "    return L.T @ psi.T\n",
    "\n",
    "def capital_phi_z_function(x, w, b, N, d, dim, L):\n",
    "    phi = np.zeros((dim*N, dim*d))\n",
    "    for i in range(N):\n",
    "        psi = capital_psi_z_function(x[:,i], w, b, d, dim, L)\n",
    "        phi[dim*i] = psi.T[0]\n",
    "        phi[dim*i+1] = psi.T[1]\n",
    "    return phi"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_vanish = x_train[:,-1]\n",
    "num_of_vanish_points = np.size(x_vanish[0])\n",
    "\n",
    "capital_phi_z = phi_function(x_vanish, w, b, num_of_vanish_points, num_of_samples, dim_of_function)\n",
    "\n",
    "# Calculate P_Psi = LL^T as done by Sindhwani\n",
    "capital_phi_z = capital_phi_z.T\n",
    "P_Psi = capital_phi_z @ np.linalg.inv(capital_phi_z.T @ capital_phi_z)@capital_phi_z.T\n",
    "P_Psi = np.eye(P_Psi[0].size) - P_Psi\n",
    "\n",
    "# Ensure P is a Hermitian positive-definite matrix given numerical noise\n",
    "P_Psi = (P_Psi + P_Psi.T) / 2 + (np.eye(P_Psi[0].size)*1e-12)\n",
    "\n",
    "L_ = np.linalg.cholesky(P_Psi)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jacobian function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def psi_jacobi(x, w, b, d, dim, L):\n",
    "    psi_vector = np.zeros(d)\n",
    "    for i in range(d):\n",
    "        psi_vector[i] = np.cos(w[:, i].T@x + b[:, i])\n",
    "    psi = np.sqrt(2/d) * np.kron(psi_vector, np.eye(dim))\n",
    "    psi_L = L.T @ psi.T\n",
    "    return psi_L.T"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alpha without constraint for vanish point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@runtime\n",
    "def alpha_approx_z(x, y, w, b, dim, d, lam, N, L):\n",
    "    phi_ = capital_phi_z_function(x, w, b, N, d, dim, L)\n",
    "    alpha_inv_part = np.linalg.inv(phi_.T @ phi_ + lam*np.eye(dim*d))\n",
    "    y_reshaped = np.array(np.ravel([y[0], y[1]], 'F'))\n",
    "    alpha = alpha_inv_part @ (phi_.T @ y_reshaped)\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_z = alpha_approx_z(x_train, y_train, w, b, dim_of_function, num_of_samples, lambda_, num_of_points, L_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learnt_model_vanish(t, x):\n",
    "    func = capital_psi_z_function(x, w, b, num_of_samples, dim_of_function, L_).T @ alpha_z\n",
    "    return func\n",
    "\n",
    "data_series = solve_ivp(learnt_model_vanish, timespan, init_condition, t_eval = t_series).y\n",
    "\n",
    "u_learnt = np.zeros(X1.shape)\n",
    "v_learnt = np.zeros(X1.shape)\n",
    "\n",
    "for (i,j), value in np.ndenumerate(X1):\n",
    "    Y_prime = learnt_model_vanish(t, np.array([value, X2[i, j]]))\n",
    "    u_learnt[i,j] = Y_prime[0]\n",
    "    v_learnt[i,j] = Y_prime[1]\n",
    "\n",
    "fig_learntmodel_closed_form = plt.figure(2)\n",
    "plt.title('Learnt model for ' + name_of_dataset + \" with vanishing point\", fontsize=14)\n",
    "plt.plot(data_series[0, :], data_series[1, :], '.', linewidth=0.1)\n",
    "plt.streamplot(X1, X2, u_learnt, v_learnt, density = 1.1, color ='gray')\n",
    "plt.xlabel('x1', fontsize=12)\n",
    "plt.ylabel('x2', fontsize=12)\n",
    "plt.scatter(0,0, color='green', s=80, zorder=10)\n",
    "plt.legend(['Trajectory', 'Streamlines'], loc = \"upper left\" , frameon = True , prop ={'size': 14 } )\n",
    "fig_learntmodel_closed_form.show()\n",
    "#plt.savefig(r\"lasa_figures/\"+name_of_dataset+\"_lasa_vanish_vanishingpoint.eps\", format =\"eps\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding alpha using finite difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@runtime\n",
    "def alpha_approx_with_constraint_z(x, y, w, b, dim, d, lam, N, mu, constraint_points, L):\n",
    "    phi_ = capital_phi_z_function(x, w, b, N, d, dim, L)\n",
    "    mu = mu*np.eye(dim)\n",
    "    problem = pc.Problem()\n",
    "    phi_param = pc.Constant('phi_', phi_)\n",
    "    lam_param = pc.Constant('lam', lam)\n",
    "    dim_param = pc.Constant('dim',dim)\n",
    "    d_param = pc.Constant('d', d)\n",
    "    mu_param = pc.Constant('mu', mu)\n",
    "    constraint_points_param = pc.Constant('constraint_points', constraint_points)\n",
    "    y_reshaped = np.array(np.ravel([y[0], y[1]], 'F'))\n",
    "    y_reshaped_param = pc.Constant('y_reshaped', y_reshaped)\n",
    "    alpha_var = pc.RealVariable('alpha_var', (d_param*dim_param, 1))\n",
    "    # Creating constraints\n",
    "    for i in range(constraint_points_param):\n",
    "        constraint_index = i*np.int64(np.floor(len(x[0])/constraint_points))\n",
    "        x_i = x[:, constraint_index]\n",
    "        gradient = np.zeros(dim)\n",
    "        jacobi_function = nd.Jacobian(psi_jacobi)\n",
    "        jacobi_ = jacobi_function(x_i, w, b, d, dim, L)\n",
    "        jacobi_param = pc.Constant('jacobi', [0, 0], (2, 2))\n",
    "        for j in range(len(alpha_var)):\n",
    "            jacobi_param += pc.Constant(jacobi_[:, :, j]) * alpha_var[j]\n",
    "        gradient = gradient + 0.5 * (jacobi_param + jacobi_param.T)\n",
    "        problem.add_constraint(gradient << mu_param)\n",
    "    obj = ((phi_param * alpha_var) - y_reshaped_param).T * ((phi_param * alpha_var) - y_reshaped_param) + lam_param*(alpha_var.T * alpha_var)\n",
    "    problem.set_objective('min', obj)\n",
    "    problem.solve(solver='mosek')\n",
    "    alpha_var = alpha_var.np.reshape((-1,))\n",
    "    return alpha_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_fdm = alpha_approx_with_constraint_z(x_train, y_train, w, b, dim_of_function, num_of_samples, lambda_, num_of_points, mu, num_of_constraint_points, L_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learnt_model_fdm(t, x):\n",
    "    func = capital_psi_z_function(x, w, b, num_of_samples, dim_of_function, L_).T @ alpha_fdm\n",
    "    return func\n",
    "\n",
    "data_series_fdm = solve_ivp(learnt_model_fdm, timespan, init_condition, t_eval = t_series).y\n",
    "\n",
    "x1 = np.linspace(lower_bound_x1-5, upper_bound_x1, num_of_points_for_plot)\n",
    "x2 = np.linspace(lower_bound_x2, upper_bound_x2+3, num_of_points_for_plot)\n",
    "\n",
    "# Base point or starting point for each vector of the vector field\n",
    "[X1, X2] = np.meshgrid(x1, x2)\n",
    "\n",
    "u_learnt_fdm = np.zeros(X1.shape)\n",
    "v_learnt_fdm = np.zeros(X1.shape)\n",
    "\n",
    "for (i,j), value in np.ndenumerate(X1):\n",
    "    Y_prime_fdm = learnt_model_fdm(t, np.array([value, X2[i, j]]))\n",
    "    u_learnt_fdm[i,j] = Y_prime_fdm[0]\n",
    "    v_learnt_fdm[i,j] = Y_prime_fdm[1]\n",
    "\n",
    "fig_learntmodel_fdm = plt.figure(3)\n",
    "plt.title(name_of_dataset + \" Shape with contraction constraints & vanishing point\", fontsize=14)\n",
    "plt.plot(data_series_fdm[0, :], data_series_fdm[1, :], '.', linewidth=0.1)\n",
    "plt.streamplot(X1, X2, u_learnt_fdm, v_learnt_fdm, density = 1.1, color ='gray')\n",
    "plt.xlabel('x1', fontsize=12)\n",
    "plt.ylabel('x2', fontsize=12)\n",
    "plt.scatter(0,0, color='green', s=80, zorder=10)\n",
    "plt.legend(['Trajectory', 'Streamlines'], loc = \"upper left\" , frameon = True , prop ={'size': 14 } )\n",
    "fig_learntmodel_fdm.show()\n",
    "#plt.savefig(r\"lasa_figures/\"+name_of_dataset+\"_lasa_vanish_fdm.eps\", format =\"eps\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reproduction Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory_error = np.zeros(num_of_test_datasets)\n",
    "\n",
    "for i in range(num_of_test_datasets):\n",
    "    demo_pos = num_of_train_datasets + i\n",
    "    actual_trajectory = dataset['demos'][0, demo_pos][0, 0][0]\n",
    "    actual_time = dataset['demos'][0, demo_pos][0, 0][1]\n",
    "    timespan = np.array([actual_time[:, 0], actual_time[:, -1]])\n",
    "    initial_condition = actual_trajectory[:, 0]\n",
    "    simulated_trajectory = solve_ivp(learnt_model_fdm, timespan, initial_condition, t_eval = actual_time[0]).y\n",
    "\n",
    "    error = np.mean(np.linalg.norm(actual_trajectory - simulated_trajectory, ord=2, axis=0) / actual_time[:, -1])\n",
    "    trajectory_error[i] = error\n",
    "\n",
    "    demo_number = demo_pos+1\n",
    "    fig_learntmodel_fdm = plt.figure(i+num_of_train_datasets)\n",
    "    plt.title('Combined plot - Learnt model for ' + name_of_dataset + ' demo ' + str(demo_number), fontsize=14)\n",
    "    plt.plot(actual_trajectory[0, :], actual_trajectory[1, :], '.', linewidth=0.1, color = 'r')\n",
    "    plt.plot(simulated_trajectory[0, :], simulated_trajectory[1, :], '.', linewidth=0.1)\n",
    "    plt.scatter(0,0, color='green', s=80, zorder=10)\n",
    "    plt.streamplot(X1, X2, u_learnt_fdm, v_learnt_fdm, density = 1.1, color ='gray')\n",
    "    plt.xlabel('x1', fontsize=12)\n",
    "    plt.ylabel('x2', fontsize=12)\n",
    "    #plt.legend(['Actual Trajectory', 'Simulated Trajectory'], loc = \"upper left\" , frameon = True , prop ={'size': 14 } )\n",
    "    fig_learntmodel_fdm.show()\n",
    "    #plt.savefig(r\"lasa_figures/\"+name_of_dataset+\"_lasa_vanish_fdm_accuracy_demo\"+str(demo_number)+\".eps\", format =\"eps\")\n",
    "\n",
    "trajectory_mean_error = np.mean(trajectory_error)\n",
    "print(trajectory_error)\n",
    "print(np.mean(trajectory_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save('RFF_parameters/w_'+name_of_dataset+'_vanish', w)\n",
    "#np.save('RFF_parameters/b_'+name_of_dataset+'_vanish', b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "364ce0aa04b075b5f9a8a87d4ba65b1dbc176d267b37027aa80213cb40fae0fc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
